{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('../../main/data/origin/train.csv')\n",
    "df_train['군구'] = df_train['시군구'].replace('대구광역시 ','',regex=True)\n",
    "df_train['사고일시'] = pd.to_datetime(df_train['사고일시'])\n",
    "df_train['월'] = df_train['사고일시'].dt.month\n",
    "df_train['시간'] = df_train['사고일시'].dt.hour\n",
    "df_train.drop(columns=['시군구'], inplace=True)\n",
    "df_train.drop(columns=['ID','사고유형 - 세부분류', '법규위반', '가해운전자 차종', '가해운전자 성별', '가해운전자 연령', '가해운전자 상해정도',\n",
    "       '피해운전자 차종', '피해운전자 성별', '피해운전자 연령', '피해운전자 상해정도','사고일시','ECLO'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['요일', '기상상태', '도로형태', '노면상태', '사고유형', '사망자수', '중상자수', '경상자수', '부상자수',\n",
       "       '군구', '월', '시간'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.get_dummies(df_train, columns = ['요일', '기상상태', '도로형태', '노면상태', '사고유형'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = df_train.drop(columns=['사망자수', '중상자수', '경상자수', '부상자수'])\n",
    "y_trn = df_train[['사망자수', '중상자수', '경상자수', '부상자수']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39609, 36), (39609,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn.shape, y_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import Optional, List\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, *args:list[np.array]):\n",
    "    assert all(args[0].shape[0] == arg.shape[0] for arg in args), \"Size mismatch.\"\n",
    "    self.data = args\n",
    "  def __getitem__(self, index):\n",
    "    return tuple(x[index] for x in self.data)\n",
    "  def __len__(self):\n",
    "    return self.data[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "activation_list = {\"sigmoid\": nn.Sigmoid(), \"relu\": nn.ReLU(), \"tanh\": nn.Tanh(), \"prelu\": nn.PReLU()}\n",
    "\n",
    "class ANN(nn.Module):\n",
    "  def __init__(self, input_dim: int=5, hidden_dim: list=[128, 128, 64, 32], activation: str=\"sigmoid\", use_drop:bool = True, drop_ratio: float=0.3):\n",
    "    super().__init__()\n",
    "    # self.embedding = nn.Linear(1,10)\n",
    "    dims = [input_dim] + hidden_dim \n",
    "    self.Identity = nn.Identity()\n",
    "    self.dropout = nn.Dropout(drop_ratio)\n",
    "    self.activation = activation_list[activation]\n",
    "    \n",
    "    model = [[nn.Linear(dims[i], dims[i+1]), self.dropout if use_drop else self.Identity, self.activation] for i in range(len(dims) - 1)]\n",
    "    output_layer = [nn.Linear(dims[-1], 4), nn.Identity()] # 1 -> 4 하고 1 -> 사망자, data unvalance가 있어도 괜찮  mse를 활용하며 가중치를 사망자, 경상자 마다 바꾸고 학습\n",
    "    # output_layer = [nn.Linear(dims[-1], 1), nn.Identity()] # Relu는 항상 양수 값일 때 주는 방법\n",
    "    self.module_list= nn.ModuleList(sum(model, []) + output_layer)\n",
    "  def forward(self, x):\n",
    "    # x = torch.concat([x[:,:4], self.embedding(x[:,4:5]),x[:,5:]])\n",
    "    for layer in self.module_list:\n",
    "         x = layer(x) # 차원 맞춰서 갯수증가 # 다른방법: linear layer하나 추가하고 다른 피쳐들 가중치를 0 으로 학습 진행 x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import MSELoss\n",
    "def train_one_epoch(\n",
    "  model:nn.Module,\n",
    "  criterion:callable,\n",
    "  optimizer:torch.optim.Optimizer,\n",
    "  data_loader:DataLoader,\n",
    "  device:str\n",
    ") -> float:\n",
    "  '''train one epoch\n",
    "\n",
    "  Args:\n",
    "      model: model\n",
    "      criterion: loss\n",
    "      optimizer: optimizer\n",
    "      data_loader: data loader\n",
    "      device: device\n",
    "  '''\n",
    "  model.train()\n",
    "  total_loss = 0.\n",
    "  for X, y in data_loader:\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    output = model(X)\n",
    "    mse = MSELoss()\n",
    "    loss = mse(output[:,0],y[:,0]) * 10 + mse(output[:,1],y[:,1]) * 5 + mse(output[:,2],y[:,2]) * 3 + mse(output[:,3],y[:,3])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item() * len(y)\n",
    "  return total_loss/len(data_loader.dataset)\n",
    "\n",
    "def evaluate(\n",
    "  model: nn.Module,\n",
    "  criterion: callable,\n",
    "  data_loader: DataLoader,\n",
    "  device: str,\n",
    "  metric: Optional[torchmetrics.metric.Metric] = None,\n",
    "  multi_metrics: List[torchmetrics.metric.Metric] = None\n",
    ") -> float:\n",
    "  '''evaluate\n",
    "\n",
    "  Args:\n",
    "      model: model\n",
    "      criterions: list of criterion functions\n",
    "      data_loader: data loader\n",
    "      device: device\n",
    "  '''\n",
    "  model.eval()\n",
    "  total_loss = 0.\n",
    "  with torch.no_grad():  # no_grad context로 감싸서 그라디언트 계산 비활성화\n",
    "    for X, y in data_loader:\n",
    "      X, y = X.to(device), y.to(device)\n",
    "      output = model(X)\n",
    "      loss = mse(output[:,0],y[:,0]) * 10 + mse(output[:,1],y[:,1]) * 5 + mse(output[:,2],y[:,2]) * 3 + mse(output[:,3],y[:,3])\n",
    "      total_loss += loss.item() * len(y)\n",
    "\n",
    "      if metric is not None:\n",
    "        # For a single metric, update it with the total output and total target\n",
    "        metric.update(output, y)\n",
    "\n",
    "      if multi_metrics is not None:\n",
    "        # For multiple metrics, update each metric with the current output and target\n",
    "        for metric in multi_metrics:\n",
    "          metric.update(output, y)\n",
    "\n",
    "    # 평가 지표 계산 시에는 스칼라인 경우에 대한 처리 추가\n",
    "    if isinstance(total_loss, torch.Tensor): # isinstance(x,y) x의 타입이 y가 맞는지 T,F\n",
    "        return total_loss.item() / len(data_loader.dataset)\n",
    "    else:\n",
    "        return total_loss / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "def kfold_cross_validation(model: nn.Module, criterion:callable, device:str, X_trn:np.array, y_trn:np.array, n_splits:int=5):\n",
    "  from sklearn.model_selection import KFold\n",
    "  from torchmetrics import MeanAbsoluteError, MeanSquaredError, MeanSquaredLogError\n",
    "  # from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_squared_log_error\n",
    "  from copy import deepcopy\n",
    "  \n",
    "  Kf = KFold(n_splits=n_splits, shuffle=True, random_state=2023)\n",
    "  nets = [deepcopy(model) for i in range(n_splits)]\n",
    "  scores = {\n",
    "  'mae': [],\n",
    "  'mse': [],\n",
    "  'msle': []\n",
    "  }\n",
    "  \n",
    "  for i, (trn_idx, val_idx) in enumerate(Kf.split(X_trn, y_trn)):\n",
    "    X, y = torch.tensor(X_trn.iloc[trn_idx].values.astype(np.float32)), torch.tensor(y_trn.iloc[trn_idx].values.astype(np.float32))\n",
    "    X_val, y_val = torch.tensor(X_trn.iloc[val_idx].values.astype(np.float32)), torch.tensor(y_trn.iloc[val_idx].values.astype(np.float32))\n",
    "\n",
    "    ds = CustomDataset(X, y)\n",
    "    ds_val = CustomDataset(X_val, y_val)\n",
    "    dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "    dl_val = DataLoader(ds_val, batch_size=len(ds_val), shuffle=False)\n",
    "\n",
    "    net = nets[i].train()\n",
    "\n",
    "    pbar = tqdm(range(30))\n",
    "    for j in pbar:\n",
    "      mae, mse, msle = MeanAbsoluteError().to(device), MeanSquaredError().to(device), MeanSquaredLogError().to(device)\n",
    "      optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "      loss = train_one_epoch(net, criterion, optimizer, dl, device)\n",
    "      loss_val = evaluate(net, criterion, dl_val, device, multi_metrics=[mae, mse, msle])\n",
    "      mae, mse, msle = mae.compute(), mse.compute(), msle.compute()\n",
    "      # Get predictions for validation set\n",
    "      # y_val_pred = net(X_val).detach().numpy()\n",
    "      pbar.set_postfix(trn_loss=loss, val_loss=loss_val)  # 진행바 우측에 진행상황 표시\n",
    "    scores[\"mae\"].append(mae.item())\n",
    "    scores[\"mse\"].append(mse.item())\n",
    "    scores[\"msle\"].append(msle.item())\n",
    "\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['사망자수', '중상자수', '경상자수', '부상자수'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\book\\ESTSOft\\work\\project\\work\\est_wassup_01\\examination\\DoYeon\\123.ipynb 셀 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/examination/DoYeon/123.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/examination/DoYeon/123.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_trn \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m사망자수\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m중상자수\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m경상자수\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m부상자수\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/examination/DoYeon/123.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_trn \u001b[39m=\u001b[39m df_train[[\u001b[39m'\u001b[39m\u001b[39m사망자수\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m중상자수\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m경상자수\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m부상자수\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/examination/DoYeon/123.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m ds \u001b[39m=\u001b[39m CustomDataset(X_trn, y_trn)\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pandas\\core\\frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   5197\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5198\u001b[0m     labels: IndexLabel \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5206\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5208\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5209\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5342\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5343\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5345\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5346\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5347\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5348\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5349\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5350\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5351\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5352\u001b[0m     )\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pandas\\core\\generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4709\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4710\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4711\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4713\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4714\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pandas\\core\\generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4751\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4752\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4753\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4754\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4756\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6992\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6990\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6991\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6992\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m.\u001b[39mtolist()\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6993\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6994\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['사망자수', '중상자수', '경상자수', '부상자수'] not found in axis\""
     ]
    }
   ],
   "source": [
    "device = torch.device(device)\n",
    "X_trn = df_train.drop(columns=['사망자수', '중상자수', '경상자수', '부상자수'])\n",
    "y_trn = df_train[['사망자수', '중상자수', '경상자수', '부상자수']]\n",
    "\n",
    "ds = CustomDataset(X_trn, y_trn)\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "\n",
    "model = ANN(X_trn.shape[-1], hidden_dim=[64,64], drop_ratio = 0.3).to(device)\n",
    "print(model)\n",
    "loss_func = nn.functional.mse_loss\n",
    "\n",
    "scores = kfold_cross_validation(model, loss_func, device, X_trn, y_trn)\n",
    "# scores_df = pd.DataFrame(scores)\n",
    "# scores_df = pd.concat([scores_df, scores_df.apply(['mean', 'std'])])\n",
    "# scores_df.to_csv(\"scores.csv\", index=False)\n",
    "# print(scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:29<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:30<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:02<00:39,  1.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\book\\ESTSOft\\work\\project\\work\\est_wassup_01\\test\\123.ipynb 셀 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/test/123.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     pbar \u001b[39m=\u001b[39m tqdm(pbar)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/test/123.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/test/123.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_one_epoch(model, loss_func, optimizer, dl, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/test/123.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m pbar\u001b[39m.\u001b[39mset_postfix(trn_loss\u001b[39m=\u001b[39mloss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/test/123.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# save pretrained weight\u001b[39;00m\n",
      "\u001b[1;32md:\\book\\ESTSOft\\work\\project\\work\\est_wassup_01\\test\\123.ipynb 셀 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/test/123.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/test/123.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/test/123.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m data_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/test/123.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/book/ESTSOft/work/project/work/est_wassup_01/test/123.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   output \u001b[39m=\u001b[39m model(X)\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m samples \u001b[39min\u001b[39;49;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\users\\zpdlc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "Kf = KFold(n_splits=5, shuffle=True, random_state=2023)\n",
    "for i, (trn_idx, val_idx) in enumerate(Kf.split(X_trn, y_trn)):\n",
    "    X, y = torch.tensor(X_trn.iloc[trn_idx].values.astype(np.float32)), torch.tensor(y_trn.iloc[trn_idx].values.astype(np.float32))\n",
    "    X_val, y_val = torch.tensor(X_trn.iloc[val_idx].values.astype(np.float32)), torch.tensor(y_trn.iloc[val_idx].values.astype(np.float32))\n",
    "\n",
    "    ds = CustomDataset(X, y)\n",
    "    ds_val = CustomDataset(X_val, y_val)\n",
    "    dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "    dl_val = DataLoader(ds_val, batch_size=len(ds_val), shuffle=False)\n",
    "\n",
    "\n",
    "    # model = ANN(X_trn.shape[-1], hidden_dim=64, activation='relu', use_dropout=True, drop_ratio = 0.3).to(device)\n",
    "\n",
    "\n",
    "    # 최종학습\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.00001)\n",
    "    pbar = range(30)\n",
    "    if True:\n",
    "        pbar = tqdm(pbar)\n",
    "    for _ in pbar:\n",
    "        loss = train_one_epoch(model, loss_func, optimizer, dl, device)\n",
    "    pbar.set_postfix(trn_loss=loss)\n",
    "\n",
    "    # save pretrained weight\n",
    "    torch.save(model.state_dict(), \"./model.pth\")\n",
    "    # final outuput with testset\n",
    "    model = ANN(input_dim=X_trn.shape[-1], hidden_dim=[64,64], activation='relu').to(device)\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "\n",
    "    result = []\n",
    "    with torch.inference_mode():\n",
    "        for X in dl_val:\n",
    "            X = X[0].to(device)\n",
    "            output = model(X).squeeze().tolist()\n",
    "            result.extend(output)\n",
    "\n",
    "    test_id = y_trn.iloc[val_idx].index.tolist()\n",
    "    result = pd.DataFrame(result)\n",
    "    result['ID'] = test_id\n",
    "    print()\n",
    "    result.rename(columns={'ID': 'ID',\n",
    "                           '1': '사망자수',\n",
    "                           '2': '중상자수', \n",
    "                           '3': '경상자수', \n",
    "                           '4': '부상자수'},inplace=True)\n",
    "    result.to_csv(\"result.csv\", index=False)\n",
    "    df['ECLO'] = (df['사망자수'] * 10) + (df['중상자수'] * 5) + (df['경상자수'] * 3) + (df['부상자수'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.mean of 0       4.739994\n",
       "1       4.618116\n",
       "2       4.293276\n",
       "3       3.858552\n",
       "4       4.599720\n",
       "          ...   \n",
       "7917    4.244061\n",
       "7918    4.718004\n",
       "7919    4.738001\n",
       "7920    4.955145\n",
       "7921    4.645133\n",
       "Name: ECLO, Length: 7922, dtype: float64>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
